{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install Required Dependencies\n!pip install stable-baselines3 gym gymnasium\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T01:23:32.530518Z","iopub.execute_input":"2024-10-30T01:23:32.531487Z","iopub.status.idle":"2024-10-30T01:23:44.396300Z","shell.execute_reply.started":"2024-10-30T01:23:32.531444Z","shell.execute_reply":"2024-10-30T01:23:44.395044Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: stable-baselines3 in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: gym in /opt/conda/lib/python3.10/site-packages (0.26.2)\nRequirement already satisfied: gymnasium in /opt/conda/lib/python3.10/site-packages (0.29.0)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (1.26.4)\nRequirement already satisfied: torch>=1.13 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.4.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (3.0.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.2.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (3.7.5)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym) (0.0.8)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (0.0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (2024.6.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 2: Import Required Libraries\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nimport numpy as np\nimport torch\nfrom dataclasses import dataclass\nimport logging\nfrom typing import Dict, Any, Tuple\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T01:23:44.398868Z","iopub.execute_input":"2024-10-30T01:23:44.399226Z","iopub.status.idle":"2024-10-30T01:23:44.405088Z","shell.execute_reply.started":"2024-10-30T01:23:44.399191Z","shell.execute_reply":"2024-10-30T01:23:44.404143Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Step 3: Setup Basic Logging\nlogging.basicConfig(\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    level=logging.INFO\n)\nlogger = logging.getLogger(__name__)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T01:23:44.406171Z","iopub.execute_input":"2024-10-30T01:23:44.406477Z","iopub.status.idle":"2024-10-30T01:23:44.416382Z","shell.execute_reply.started":"2024-10-30T01:23:44.406446Z","shell.execute_reply":"2024-10-30T01:23:44.415502Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Step 4: Configuration Class\n@dataclass\nclass PPOConfig:\n    \"\"\"Configuration for PPO training\"\"\"\n    env_id: str = \"CartPole-v1\"\n    n_envs: int = 1\n    n_timesteps: int = 10000\n    save_path: str = \"./ppo_model\"\n    policy: str = \"MlpPolicy\"\n    learning_rate: float = 3e-4\n    n_steps: int = 2048\n    batch_size: int = 64\n    n_epochs: int = 10\n    gamma: float = 0.99\n    verbose: int = 1\n    device: str = \"auto\"\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T01:23:44.417546Z","iopub.execute_input":"2024-10-30T01:23:44.417926Z","iopub.status.idle":"2024-10-30T01:23:44.428415Z","shell.execute_reply.started":"2024-10-30T01:23:44.417884Z","shell.execute_reply":"2024-10-30T01:23:44.427597Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Step 5: Environment Setup Class\nclass EnvironmentManager:\n    def __init__(self, config: PPOConfig):\n        self.config = config\n        self.env = None\n        \n    def create_env(self):\n        \"\"\"Create and configure the environment\"\"\"\n        logger.info(f\"Creating environment: {self.config.env_id}\")\n        try:\n            # Create a single environment for evaluation\n            self.env = gym.make(self.config.env_id)\n            logger.info(\"Environment created successfully\")\n            return self.env\n        except Exception as e:\n            logger.error(f\"Error creating environment: {str(e)}\")\n            raise\n\n    def create_vec_env(self):\n        \"\"\"Create vectorized environment for training\"\"\"\n        logger.info(f\"Creating vectorized environment: {self.config.env_id}\")\n        try:\n            self.env = make_vec_env(\n                self.config.env_id,\n                n_envs=self.config.n_envs\n            )\n            logger.info(\"Vectorized environment created successfully\")\n            return self.env\n        except Exception as e:\n            logger.error(f\"Error creating vectorized environment: {str(e)}\")\n            raise\n\n    def close_env(self):\n        \"\"\"Close the environment\"\"\"\n        if self.env:\n            self.env.close()\n            logger.info(\"Environment closed\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T01:23:44.430366Z","iopub.execute_input":"2024-10-30T01:23:44.430645Z","iopub.status.idle":"2024-10-30T01:23:44.443418Z","shell.execute_reply.started":"2024-10-30T01:23:44.430614Z","shell.execute_reply":"2024-10-30T01:23:44.442496Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Step 6: PPO Trainer Class\nclass PPOTrainer:\n    def __init__(self, config: PPOConfig):\n        self.config = config\n        self.model = None\n        self.env_manager = EnvironmentManager(config)\n        \n    def setup_model(self, env) -> PPO:\n        \"\"\"Initialize the PPO model\"\"\"\n        logger.info(\"Initializing PPO model\")\n        try:\n            self.model = PPO(\n                policy=self.config.policy,\n                env=env,\n                learning_rate=self.config.learning_rate,\n                n_steps=self.config.n_steps,\n                batch_size=self.config.batch_size,\n                n_epochs=self.config.n_epochs,\n                gamma=self.config.gamma,\n                verbose=self.config.verbose,\n                device=self.config.device\n            )\n            logger.info(\"PPO model initialized successfully\")\n            return self.model\n        except Exception as e:\n            logger.error(f\"Error initializing model: {str(e)}\")\n            raise\n\n    def train(self) -> Dict[str, Any]:\n        \"\"\"Train the PPO model\"\"\"\n        logger.info(\"Starting training process\")\n        try:\n            # Create vectorized environment for training\n            env = self.env_manager.create_vec_env()\n            \n            # Setup model if not already initialized\n            if self.model is None:\n                self.model = self.setup_model(env)\n            \n            # Train the model\n            logger.info(f\"Training for {self.config.n_timesteps} timesteps\")\n            self.model.learn(\n                total_timesteps=self.config.n_timesteps\n            )\n            \n            # Save the trained model\n            self.save_model()\n            \n            return {\n                \"status\": \"success\",\n                \"timesteps_trained\": self.config.n_timesteps\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error during training: {str(e)}\")\n            raise\n        finally:\n            self.env_manager.close_env()\n\n    def save_model(self):\n        \"\"\"Save the trained model\"\"\"\n        if self.model:\n            logger.info(f\"Saving model to {self.config.save_path}\")\n            self.model.save(self.config.save_path)\n            logger.info(\"Model saved successfully\")\n\n    def load_model(self) -> PPO:\n        \"\"\"Load a trained model\"\"\"\n        logger.info(f\"Loading model from {self.config.save_path}\")\n        try:\n            self.model = PPO.load(self.config.save_path)\n            logger.info(\"Model loaded successfully\")\n            return self.model\n        except Exception as e:\n            logger.error(f\"Error loading model: {str(e)}\")\n            raise\n\n    def evaluate(self, n_eval_episodes: int = 10) -> Tuple[float, float]:\n        \"\"\"Evaluate the trained model\"\"\"\n        logger.info(f\"Evaluating model for {n_eval_episodes} episodes\")\n        try:\n            # Create a single environment for evaluation\n            env = self.env_manager.create_env()\n            episode_rewards = []\n            episode_lengths = []\n\n            for episode in range(n_eval_episodes):\n                obs, _ = env.reset()\n                done = False\n                truncated = False\n                episode_reward = 0\n                episode_length = 0\n\n                while not (done or truncated):\n                    action, _ = self.model.predict(obs, deterministic=True)\n                    obs, reward, done, truncated, _ = env.step(action)\n                    episode_reward += reward\n                    episode_length += 1\n\n                episode_rewards.append(episode_reward)\n                episode_lengths.append(episode_length)\n\n            mean_reward = np.mean(episode_rewards)\n            mean_length = np.mean(episode_lengths)\n\n            logger.info(f\"Mean reward: {mean_reward:.2f}\")\n            logger.info(f\"Mean episode length: {mean_length:.2f}\")\n\n            return mean_reward, mean_length\n\n        except Exception as e:\n            logger.error(f\"Error during evaluation: {str(e)}\")\n            raise\n        finally:\n            self.env_manager.close_env()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T01:23:44.444698Z","iopub.execute_input":"2024-10-30T01:23:44.445078Z","iopub.status.idle":"2024-10-30T01:23:44.464164Z","shell.execute_reply.started":"2024-10-30T01:23:44.445037Z","shell.execute_reply":"2024-10-30T01:23:44.463258Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Step 7: Main Training Function\ndef main():\n    # Set random seeds for reproducibility\n    torch.manual_seed(42)\n    np.random.seed(42)\n\n    # Initialize configuration\n    config = PPOConfig()\n    \n    try:\n        # Initialize trainer\n        trainer = PPOTrainer(config)\n        \n        # Train the model\n        logger.info(\"Starting training...\")\n        results = trainer.train()\n        logger.info(\"Training completed successfully\")\n        \n        # Evaluate the trained model\n        logger.info(\"Starting evaluation...\")\n        mean_reward, mean_length = trainer.evaluate()\n        logger.info(\"Evaluation completed\")\n        \n        # Print final results\n        print(\"\\nTraining Results:\")\n        print(f\"Total timesteps: {results['timesteps_trained']}\")\n        print(\"\\nEvaluation Results:\")\n        print(f\"Mean reward: {mean_reward:.2f}\")\n        print(f\"Mean episode length: {mean_length:.2f}\")\n        \n    except Exception as e:\n        logger.error(f\"An error occurred: {str(e)}\")\n        raise\n\n# Step 8: Run Training\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T01:23:44.465373Z","iopub.execute_input":"2024-10-30T01:23:44.465682Z","iopub.status.idle":"2024-10-30T01:24:08.997738Z","shell.execute_reply.started":"2024-10-30T01:23:44.465652Z","shell.execute_reply":"2024-10-30T01:24:08.996807Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Using cuda device\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 21.8     |\n|    ep_rew_mean     | 21.8     |\n| time/              |          |\n|    fps             | 727      |\n|    iterations      | 1        |\n|    time_elapsed    | 2        |\n|    total_timesteps | 2048     |\n---------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 30          |\n|    ep_rew_mean          | 30          |\n| time/                   |             |\n|    fps                  | 582         |\n|    iterations           | 2           |\n|    time_elapsed         | 7           |\n|    total_timesteps      | 4096        |\n| train/                  |             |\n|    approx_kl            | 0.008727467 |\n|    clip_fraction        | 0.0797      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.687      |\n|    explained_variance   | -0.00464    |\n|    learning_rate        | 0.0003      |\n|    loss                 | 6.55        |\n|    n_updates            | 10          |\n|    policy_gradient_loss | -0.0116     |\n|    value_loss           | 51.8        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 38.9        |\n|    ep_rew_mean          | 38.9        |\n| time/                   |             |\n|    fps                  | 549         |\n|    iterations           | 3           |\n|    time_elapsed         | 11          |\n|    total_timesteps      | 6144        |\n| train/                  |             |\n|    approx_kl            | 0.008464789 |\n|    clip_fraction        | 0.0557      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.665      |\n|    explained_variance   | 0.116       |\n|    learning_rate        | 0.0003      |\n|    loss                 | 13.2        |\n|    n_updates            | 20          |\n|    policy_gradient_loss | -0.0173     |\n|    value_loss           | 42.7        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 47.5        |\n|    ep_rew_mean          | 47.5        |\n| time/                   |             |\n|    fps                  | 531         |\n|    iterations           | 4           |\n|    time_elapsed         | 15          |\n|    total_timesteps      | 8192        |\n| train/                  |             |\n|    approx_kl            | 0.009743036 |\n|    clip_fraction        | 0.089       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.63       |\n|    explained_variance   | 0.228       |\n|    learning_rate        | 0.0003      |\n|    loss                 | 27.9        |\n|    n_updates            | 30          |\n|    policy_gradient_loss | -0.0197     |\n|    value_loss           | 56.6        |\n-----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 59.5       |\n|    ep_rew_mean          | 59.5       |\n| time/                   |            |\n|    fps                  | 521        |\n|    iterations           | 5          |\n|    time_elapsed         | 19         |\n|    total_timesteps      | 10240      |\n| train/                  |            |\n|    approx_kl            | 0.00589127 |\n|    clip_fraction        | 0.056      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.608     |\n|    explained_variance   | 0.233      |\n|    learning_rate        | 0.0003     |\n|    loss                 | 32.9       |\n|    n_updates            | 40         |\n|    policy_gradient_loss | -0.0167    |\n|    value_loss           | 69.1       |\n----------------------------------------\n\nTraining Results:\nTotal timesteps: 10000\n\nEvaluation Results:\nMean reward: 446.50\nMean episode length: 446.50\n","output_type":"stream"}]}]}