{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"LLaMA Factory Supervised Fine-tuning for Kaggle\"\"\"\n\n# Disable wandb logging\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-10-29T02:54:14.948681Z","iopub.execute_input":"2024-10-29T02:54:14.949107Z","iopub.status.idle":"2024-10-29T02:54:14.959488Z","shell.execute_reply.started":"2024-10-29T02:54:14.949051Z","shell.execute_reply":"2024-10-29T02:54:14.958616Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Install dependencies and authenticate with HuggingFace\n!pip install -q --upgrade huggingface_hub\n!huggingface-cli login --token \"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\"  # Replace with your actual token","metadata":{"execution":{"iopub.status.busy":"2024-10-29T02:54:44.440045Z","iopub.execute_input":"2024-10-29T02:54:44.440473Z","iopub.status.idle":"2024-10-29T02:54:57.473238Z","shell.execute_reply.started":"2024-10-29T02:54:44.440417Z","shell.execute_reply":"2024-10-29T02:54:57.472236Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution -ax (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ax (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `sjsu assignments` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `sjsu assignments`\n","output_type":"stream"}]},{"cell_type":"code","source":"# Clean up existing directory and reinstall\n!rm -rf LLaMA-Factory\n!pip install -q --upgrade huggingface_hub\n!huggingface-cli login --token \"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\"  # Replace with your actual token\n!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n%cd LLaMA-Factory\n!pip install -q -e .[torch,bitsandbytes]","metadata":{"execution":{"iopub.status.busy":"2024-10-29T02:58:33.075112Z","iopub.execute_input":"2024-10-29T02:58:33.075965Z","iopub.status.idle":"2024-10-29T02:59:43.078556Z","shell.execute_reply.started":"2024-10-29T02:58:33.075919Z","shell.execute_reply":"2024-10-29T02:59:43.077421Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution -ax (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ax (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0mThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `sjsu assignments` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `sjsu assignments`\nCloning into 'LLaMA-Factory'...\nremote: Enumerating objects: 315, done.\u001b[K\nremote: Counting objects: 100% (315/315), done.\u001b[K\nremote: Compressing objects: 100% (238/238), done.\u001b[K\nremote: Total 315 (delta 78), reused 194 (delta 64), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (315/315), 9.03 MiB | 18.39 MiB/s, done.\nResolving deltas: 100% (78/78), done.\n/kaggle/working/LLaMA-Factory/LLaMA-Factory\n\u001b[33mWARNING: Ignoring invalid distribution -ax (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ax (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Verify GPU availability\nimport torch\ntry:\n    assert torch.cuda.is_available() is True\n    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\nexcept AssertionError:\n    print(\"Please set up a GPU runtime\")\n    raise\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T03:01:13.028519Z","iopub.execute_input":"2024-10-29T03:01:13.029321Z","iopub.status.idle":"2024-10-29T03:01:15.637416Z","shell.execute_reply.started":"2024-10-29T03:01:13.029275Z","shell.execute_reply":"2024-10-29T03:01:15.636451Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"GPU is available: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Update identity dataset\nimport json\n\nNAME = \"Llama-3\"\nAUTHOR = \"LLaMA Factory\"\n\nwith open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n    dataset = json.load(f)\n\nfor sample in dataset:\n    sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n\nwith open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(dataset, f, indent=2, ensure_ascii=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T03:01:20.977906Z","iopub.execute_input":"2024-10-29T03:01:20.978998Z","iopub.status.idle":"2024-10-29T03:01:20.989091Z","shell.execute_reply.started":"2024-10-29T03:01:20.978941Z","shell.execute_reply":"2024-10-29T03:01:20.988127Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Training arguments\ntraining_args = {\n    \"stage\": \"sft\",\n    \"do_train\": True,\n    \"model_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"dataset\": \"identity,alpaca_en_demo\",\n    \"template\": \"llama3\",\n    \"finetuning_type\": \"lora\",\n    \"lora_target\": \"all\",\n    \"output_dir\": \"llama3_lora\",\n    \n    # Training hyperparameters\n    \"per_device_train_batch_size\": 2,\n    \"gradient_accumulation_steps\": 4,\n    \"learning_rate\": 5e-5,\n    \"num_train_epochs\": 3.0,\n    \"max_samples\": 500,\n    \"max_grad_norm\": 1.0,\n    \n    # LoRA specific settings\n    \"loraplus_lr_ratio\": 16.0,\n    \n    # Optimization settings\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.1,\n    \"fp16\": True,\n    \n    # Logging and saving\n    \"logging_steps\": 10,\n    \"save_steps\": 1000,\n    \n    # Disable wandb\n    \"report_to\": \"none\"\n}\n\n# Save training configuration\njson.dump(training_args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T03:01:29.977282Z","iopub.execute_input":"2024-10-29T03:01:29.977686Z","iopub.status.idle":"2024-10-29T03:01:29.985253Z","shell.execute_reply.started":"2024-10-29T03:01:29.977645Z","shell.execute_reply":"2024-10-29T03:01:29.984153Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Start training\n!llamafactory-cli train train_llama3.json\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T03:01:36.348865Z","iopub.execute_input":"2024-10-29T03:01:36.349537Z","iopub.status.idle":"2024-10-29T04:06:39.644221Z","shell.execute_reply.started":"2024-10-29T03:01:36.349492Z","shell.execute_reply":"2024-10-29T04:06:39.642904Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"10/29/2024 03:02:07 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\nconfig.json: 100%|█████████████████████████| 1.26k/1.26k [00:00<00:00, 6.85MB/s]\n[INFO|configuration_utils.py:672] 2024-10-29 03:02:08,046 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-10-29 03:02:08,048 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\ntokenizer_config.json: 100%|███████████████| 51.1k/51.1k [00:00<00:00, 80.5MB/s]\ntokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 11.2MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 345/345 [00:00<00:00, 3.11MB/s]\n[INFO|tokenization_utils_base.py:2214] 2024-10-29 03:02:10,613 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer.json\n[INFO|tokenization_utils_base.py:2214] 2024-10-29 03:02:10,613 >> loading file tokenizer.model from cache at None\n[INFO|tokenization_utils_base.py:2214] 2024-10-29 03:02:10,613 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2214] 2024-10-29 03:02:10,613 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/special_tokens_map.json\n[INFO|tokenization_utils_base.py:2214] 2024-10-29 03:02:10,613 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2478] 2024-10-29 03:02:11,189 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[INFO|configuration_utils.py:672] 2024-10-29 03:02:12,065 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-10-29 03:02:12,066 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n[INFO|tokenization_utils_base.py:2214] 2024-10-29 03:02:12,286 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer.json\n[INFO|tokenization_utils_base.py:2214] 2024-10-29 03:02:12,286 >> loading file tokenizer.model from cache at None\n[INFO|tokenization_utils_base.py:2214] 2024-10-29 03:02:12,287 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2214] 2024-10-29 03:02:12,287 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/special_tokens_map.json\n[INFO|tokenization_utils_base.py:2214] 2024-10-29 03:02:12,287 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2478] 2024-10-29 03:02:12,815 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n10/29/2024 03:02:12 - WARNING - llamafactory.model.loader - Processor was not found: 'LlamaConfig' object has no attribute 'vision_config'.\n10/29/2024 03:02:12 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n10/29/2024 03:02:12 - INFO - llamafactory.data.loader - Loading dataset identity.json...\nGenerating train split: 91 examples [00:00, 1262.43 examples/s]\nConverting format of dataset: 100%|████| 91/91 [00:00<00:00, 5011.71 examples/s]\n10/29/2024 03:02:13 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...\nGenerating train split: 1000 examples [00:00, 60280.31 examples/s]\nConverting format of dataset: 100%|██| 500/500 [00:00<00:00, 9364.54 examples/s]\nRunning tokenizer on dataset: 100%|██| 591/591 [00:00<00:00, 1361.23 examples/s]\ntraining example:\ninput_ids:\n[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\ninputs:\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nhi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nHello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\nlabel_ids:\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\nlabels:\nHello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n[INFO|configuration_utils.py:672] 2024-10-29 03:02:15,187 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-10-29 03:02:15,189 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n10/29/2024 03:02:15 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n[WARNING|quantization_config.py:400] 2024-10-29 03:02:15,580 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\nmodel.safetensors: 100%|███████████████████| 5.70G/5.70G [03:39<00:00, 25.9MB/s]\n[INFO|modeling_utils.py:3732] 2024-10-29 03:05:55,868 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/model.safetensors\n[INFO|modeling_utils.py:1622] 2024-10-29 03:05:55,949 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n[INFO|configuration_utils.py:1099] 2024-10-29 03:05:55,954 >> Generate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"pad_token_id\": 128255\n}\n\n[INFO|modeling_utils.py:4574] 2024-10-29 03:06:00,041 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n\n[INFO|modeling_utils.py:4582] 2024-10-29 03:06:00,042 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\ngeneration_config.json: 100%|██████████████████| 220/220 [00:00<00:00, 1.61MB/s]\n[INFO|configuration_utils.py:1054] 2024-10-29 03:06:00,495 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/generation_config.json\n[INFO|configuration_utils.py:1099] 2024-10-29 03:06:00,495 >> Generate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    128001,\n    128009\n  ],\n  \"max_length\": 8192,\n  \"pad_token_id\": 128255,\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\n10/29/2024 03:06:00 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n10/29/2024 03:06:00 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n10/29/2024 03:06:00 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n10/29/2024 03:06:00 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n10/29/2024 03:06:00 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,gate_proj,o_proj,up_proj,q_proj,v_proj,down_proj\n10/29/2024 03:06:01 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n[INFO|trainer.py:667] 2024-10-29 03:06:01,323 >> Using auto half precision backend\n10/29/2024 03:06:01 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n[INFO|trainer.py:2243] 2024-10-29 03:06:02,252 >> ***** Running training *****\n[INFO|trainer.py:2244] 2024-10-29 03:06:02,252 >>   Num examples = 591\n[INFO|trainer.py:2245] 2024-10-29 03:06:02,252 >>   Num Epochs = 3\n[INFO|trainer.py:2246] 2024-10-29 03:06:02,252 >>   Instantaneous batch size per device = 2\n[INFO|trainer.py:2249] 2024-10-29 03:06:02,252 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n[INFO|trainer.py:2250] 2024-10-29 03:06:02,252 >>   Gradient Accumulation steps = 4\n[INFO|trainer.py:2251] 2024-10-29 03:06:02,252 >>   Total optimization steps = 222\n[INFO|trainer.py:2252] 2024-10-29 03:06:02,261 >>   Number of trainable parameters = 20,971,520\n  0%|                                                   | 0/222 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n{'loss': 1.3718, 'grad_norm': 0.702006995677948, 'learning_rate': 2.173913043478261e-05, 'epoch': 0.14}\n{'loss': 1.2243, 'grad_norm': 2.0093157291412354, 'learning_rate': 4.347826086956522e-05, 'epoch': 0.27}\n{'loss': 1.0243, 'grad_norm': 0.7221777439117432, 'learning_rate': 4.98475042744222e-05, 'epoch': 0.41}\n{'loss': 1.0829, 'grad_norm': 1.1157007217407227, 'learning_rate': 4.910506156279029e-05, 'epoch': 0.54}\n{'loss': 1.0134, 'grad_norm': 1.366124153137207, 'learning_rate': 4.7763104379936555e-05, 'epoch': 0.68}\n{'loss': 1.0054, 'grad_norm': 0.9717888832092285, 'learning_rate': 4.585500840294794e-05, 'epoch': 0.81}\n{'loss': 1.0704, 'grad_norm': 1.1565380096435547, 'learning_rate': 4.342822968779448e-05, 'epoch': 0.95}\n{'loss': 0.8445, 'grad_norm': 2.032998561859131, 'learning_rate': 4.054312439471239e-05, 'epoch': 1.08}\n{'loss': 0.7175, 'grad_norm': 0.48301100730895996, 'learning_rate': 3.727144767643984e-05, 'epoch': 1.22}\n{'loss': 0.7947, 'grad_norm': 1.006842017173767, 'learning_rate': 3.369456906329956e-05, 'epoch': 1.35}\n{'loss': 0.8139, 'grad_norm': 1.6575757265090942, 'learning_rate': 2.990144873009946e-05, 'epoch': 1.49}\n{'loss': 0.7006, 'grad_norm': 0.7767294645309448, 'learning_rate': 2.5986424976906322e-05, 'epoch': 1.62}\n{'loss': 0.6905, 'grad_norm': 1.4802043437957764, 'learning_rate': 2.2046867951027303e-05, 'epoch': 1.76}\n{'loss': 0.7303, 'grad_norm': 0.5583033561706543, 'learning_rate': 1.8180757964234924e-05, 'epoch': 1.89}\n{'loss': 0.6042, 'grad_norm': 0.7801551818847656, 'learning_rate': 1.4484248634655401e-05, 'epoch': 2.03}\n{'loss': 0.5637, 'grad_norm': 1.0270655155181885, 'learning_rate': 1.1049275460163999e-05, 'epoch': 2.16}\n{'loss': 0.5478, 'grad_norm': 0.6913489699363708, 'learning_rate': 7.961269300209159e-06, 'epoch': 2.3}\n{'loss': 0.552, 'grad_norm': 1.0162893533706665, 'learning_rate': 5.297031633820193e-06, 'epoch': 2.43}\n{'loss': 0.5392, 'grad_norm': 1.2169102430343628, 'learning_rate': 3.1228244380351602e-06, 'epoch': 2.57}\n{'loss': 0.4358, 'grad_norm': 0.639249861240387, 'learning_rate': 1.4927221931831131e-06, 'epoch': 2.7}\n{'loss': 0.4932, 'grad_norm': 0.88512122631073, 'learning_rate': 4.472670021254899e-07, 'epoch': 2.84}\n{'loss': 0.5191, 'grad_norm': 1.0819408893585205, 'learning_rate': 1.2460271845654569e-08, 'epoch': 2.97}\n100%|███████████████████████████████████████| 222/222 [1:00:31<00:00, 13.99s/it][INFO|trainer.py:3705] 2024-10-29 04:06:34,080 >> Saving model checkpoint to llama3_lora/checkpoint-222\n[INFO|configuration_utils.py:672] 2024-10-29 04:06:34,706 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-10-29 04:06:34,708 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n[INFO|tokenization_utils_base.py:2649] 2024-10-29 04:06:34,916 >> tokenizer config file saved in llama3_lora/checkpoint-222/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2658] 2024-10-29 04:06:34,917 >> Special tokens file saved in llama3_lora/checkpoint-222/special_tokens_map.json\n[INFO|trainer.py:2505] 2024-10-29 04:06:35,545 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 3633.2836, 'train_samples_per_second': 0.488, 'train_steps_per_second': 0.061, 'train_loss': 0.784950852394104, 'epoch': 3.0}\n100%|███████████████████████████████████████| 222/222 [1:00:33<00:00, 16.37s/it]\n[INFO|trainer.py:3705] 2024-10-29 04:06:35,547 >> Saving model checkpoint to llama3_lora\n[INFO|configuration_utils.py:672] 2024-10-29 04:06:36,040 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-10-29 04:06:36,042 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n[INFO|tokenization_utils_base.py:2649] 2024-10-29 04:06:36,247 >> tokenizer config file saved in llama3_lora/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2658] 2024-10-29 04:06:36,247 >> Special tokens file saved in llama3_lora/special_tokens_map.json\n***** train metrics *****\n  epoch                    =        3.0\n  total_flos               = 16222836GF\n  train_loss               =      0.785\n  train_runtime            = 1:00:33.28\n  train_samples_per_second =      0.488\n  train_steps_per_second   =      0.061\n[INFO|modelcard.py:449] 2024-10-29 04:06:36,559 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n","output_type":"stream"}]},{"cell_type":"code","source":"# After training, save the trained LoRA adapter\nexport_args = {\n    \"model_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"adapter_name_or_path\": \"llama3_lora\",\n    \"template\": \"llama3\",\n    \"finetuning_type\": \"lora\",\n    \"export_dir\": \"llama3_lora_merged\",\n    \"export_size\": 2,                    # Size in GB of each shard\n    \"export_device\": \"cpu\"               # Use CPU for merging to avoid OOM\n}\n\n# Save export configuration\njson.dump(export_args, open(\"export_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n\n# Export the model\n!llamafactory-cli export export_llama3.json\n\nprint(\"\\nTraining and export completed!\")\nprint(\"Your trained LoRA adapters are saved in: llama3_lora/\")\nprint(\"The merged model (if successful) is saved in: llama3_lora_merged/\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T04:06:52.878411Z","iopub.execute_input":"2024-10-29T04:06:52.878842Z","iopub.status.idle":"2024-10-29T04:07:09.564932Z","shell.execute_reply.started":"2024-10-29T04:06:52.878803Z","shell.execute_reply":"2024-10-29T04:07:09.563801Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/opt/conda/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1484, in _raise_on_head_call_error\n    raise head_call_error\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1376, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1296, in get_hf_file_metadata\n    r = _request_wrapper(\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 277, in _request_wrapper\n    response = _request_wrapper(\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 301, in _request_wrapper\n    hf_raise_for_status(response)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n    raise _format(GatedRepoError, message, response) from e\nhuggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-67205f6b-7ca9f6683475462e6c870243;2aec9dd9-0809-42e4-ba63-6a9043282ccf)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct to ask for access.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/bin/llamafactory-cli\", line 8, in <module>\n    sys.exit(main())\n  File \"/kaggle/working/LLaMA-Factory/LLaMA-Factory/src/llamafactory/cli.py\", line 87, in main\n    export_model()\n  File \"/kaggle/working/LLaMA-Factory/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 72, in export_model\n    tokenizer_module = load_tokenizer(model_args)\n  File \"/kaggle/working/LLaMA-Factory/LLaMA-Factory/src/llamafactory/model/loader.py\", line 69, in load_tokenizer\n    config = load_config(model_args)\n  File \"/kaggle/working/LLaMA-Factory/LLaMA-Factory/src/llamafactory/model/loader.py\", line 119, in load_config\n    return AutoConfig.from_pretrained(model_args.model_name_or_path, **init_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1006, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 567, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 626, in _get_config_dict\n    resolved_config_file = cached_file(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n    raise EnvironmentError(\nOSError: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n403 Client Error. (Request ID: Root=1-67205f6b-7ca9f6683475462e6c870243;2aec9dd9-0809-42e4-ba63-6a9043282ccf)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct to ask for access.\n\nTraining and export completed!\nYour trained LoRA adapters are saved in: llama3_lora/\nThe merged model (if successful) is saved in: llama3_lora_merged/\n","output_type":"stream"}]}]}